{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Tcw-B4_9PBO"
      },
      "outputs": [],
      "source": [
        "apifrom fastapi import FastAPI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "#langserve is a LangChain extension library. It helps you expose your LangChain chains as REST API endpoints automatically.\n",
        "#add_routes is a helper function inside langserve.It registers your LangChain chain (or multiple chains) as API endpoints on your FastAPI app.\n",
        "from langserve import add_routes\n",
        "#uvicorn is a lightweight ASGI web server for running FastAPI (or any ASGI-based app).\n",
        "#Without uvicorn, your FastAPI app is just Python code — uvicorn actually starts the web server to listen for incoming HTTP requests.\n",
        "import uvicorn\n",
        "import os\n",
        "from langchain_community.llms import Ollama\n",
        "from dotenv import load_dotenv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "load_dotenv()\n",
        "\n",
        "os.environ['OPENAI_API_KEY']=os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "#You are initializing a FastAPI application object, stored in variable app. This object will hold your entire API server.\n",
        "app=FastAPI(\n",
        "    title=\"Langchain Server\", #The name of your API (will show in docs)\n",
        "    version=\"1.0\",         #Version number of your API\n",
        "    decsription=\"A simple API Server\"    #Description shown in auto-generated documentation (Swagger UI)\n",
        "\n",
        ")\n"
      ],
      "metadata": {
        "id": "uL08MDc19P9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "add_routes(   #From langserve — automatically exposes a LangChain chain as a REST API route in FastAPI.\n",
        "    app, # \tYour FastAPI app object you created earlier (app = FastAPI())\n",
        "    ChatOpenAI(),  #You are initializing a LangChain OpenAI model (default uses gpt-3.5-turbo unless you specify).\n",
        "    path=\"/openai\"  #The API endpoint path. This means you can send requests to http://localhost:8000/openai/invoke.\n",
        ")\n",
        "model=ChatOpenAI()\n",
        "##ollama llama2\n",
        "llm=Ollama(model=\"llama2\")\n"
      ],
      "metadata": {
        "id": "c9yArHf3Dlq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tool\tRole\n",
        "\n",
        "uvicorn-\tRuns your server (ASGI server)\n",
        "\n",
        "FastAPI-\tBuilds your API + Swagger UI\n",
        "\n",
        "langserve-\tExposes LangChain chains as routes"
      ],
      "metadata": {
        "id": "IC8piJezHOyu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "prompt1=ChatPromptTemplate.from_template(\"Write me an essay about {topic} with 100 words\")\n",
        "prompt2=ChatPromptTemplate.from_template(\"Write me an poem about {topic} for a 5 years child with 100 words\")\n",
        "\n",
        "add_routes(\n",
        "    app,\n",
        "    prompt1|model,   # OpenAI ChatOpenAI() model.\n",
        "    path=\"/essay\"\n",
        "\n",
        "\n",
        ")\n",
        "\n",
        "add_routes(\n",
        "    app,\n",
        "    prompt2|llm,   # Ollama(model=\"llama2\") model.\n",
        "    path=\"/poem\"\n",
        "\n",
        "\n",
        ")\n"
      ],
      "metadata": {
        "id": "_QghxYptDv0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__==\"__main__\":\n",
        "    uvicorn.run(app,host=\"localhost\",port=8000)\n"
      ],
      "metadata": {
        "id": "TUiwM2EZDwN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KdQQLiEgIAM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#creating streamlit app for response\n"
      ],
      "metadata": {
        "id": "Dj9UaWe0JQPZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import streamlit as st\n",
        "\n",
        "def get_openai_response(input_text):\n",
        "    response=requests.post(\"http://localhost:8000/essay/invoke\",   #Sends data to your LangChain API\n",
        "    json={'input':{'topic':input_text}})   #The input for your prompt\n",
        "\n",
        "    return response.json()['output']['content']     #Extracts the generated essay\n",
        "\n",
        "    def get_ollama_response(input_text):\n",
        "    response=requests.post(\n",
        "    \"http://localhost:8000/poem/invoke\",\n",
        "    json={'input':{'topic':input_text}})\n",
        "\n",
        "    return response.json()['output']\n",
        "\n",
        "    ## streamlit framework\n",
        "\n",
        "st.title('Langchain Demo With LLAMA2 API')\n",
        "input_text=st.text_input(\"Write an essay on\")\n",
        "input_text1=st.text_input(\"Write a poem on\")\n",
        "\n",
        "if input_text:\n",
        "    st.write(get_openai_response(input_text))\n",
        "\n",
        "if input_text1:\n",
        "    st.write(get_ollama_response(input_text1))\n"
      ],
      "metadata": {
        "id": "AZp266NYJUlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ocBxnWAZKatd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Kk3NudXHKm2o"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O0OYi0wKKnz_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}